---
title: "p8105_hw5_bk2959"
author: "Stella Koo"
date: "2024-11-04"
output: github_document
---
## Problem 1
### Birthday Problem
```{r}
birthday_problem = function(n) {
  
  birthdays = sample(1:365, n, replace = TRUE)
  
  if (length(birthdays) != length(unique(birthdays))) {
    return(TRUE)  
  } else {
    return(FALSE)
  }
}
```

```{r}
birthday_problem(15)
```

## Problem 2
### Simulation: One-Sample T-test
```{r message = FALSE}
library(tidyverse)
set.seed(1)

simulation = function(n = 30, mu, sigma = 5) {
  
  data = rnorm(n, mu, sigma)
  
  t_test = t.test(data) |>
    broom::tidy()
  
  sim_data = tibble(
    mu_hat = t_test$estimate,
    p_value = t_test$p.value)

}

sim_result_df = function(n = 30, mu, sigma = 5) {
  
  results = map(1:5000, ~simulation(n = n, mu = mu, sigma = sigma)) |>
    bind_rows()
  
  return(results)
  
}
```

### Mean = 0
```{r}
sim_result_df(mu = 0)
```

### Mean = {1, 2, 3, 4, 5, 6}
```{r}
mu_values = 1:6
mu_simulations = tibble(
  mu = mu_values,
  results = map(mu_values, ~sim_result_df(n = 30, mu = .x, sigma = 5)) 
) |>
  unnest(cols = c(results))
```

### Effect Size & Power
```{r}
power_results_plot = mu_simulations |> 
  group_by(mu) |>
  summarise(power = sum(p_value < 0.05) / 5000) |>
  ggplot(aes(x = mu, y = power)) +
  geom_smooth(se = FALSE) +
  labs(title = "True Mean vs Power", x = "True Mean (µ)", y = "Power")

power_results_plot
```

As the mean of a distribution increases, the power of a statistical test also increases, reaching a maximum of 1, where it levels off. Similarly, as the effect size increases, power increases until it reaches a plateau.

### Estimated Mean vs. True Mean: Overall and Null-Rejected Samples
```{r}
estimate_mu = mu_simulations |>
  group_by(mu) |>
  summarise(avg_mu_hat = mean(mu_hat))

rejected_mu = mu_simulations |>
  group_by(mu) |>
  filter(p_value < 0.05) |>
  summarise(avg_mu_hat = mean(mu_hat))
  
mu_results_plot = ggplot() +
  geom_point(data = estimate_mu, aes(x = mu, y = avg_mu_hat), color = "deepskyblue") +
  geom_point(data = rejected_mu, aes(x = mu, y = avg_mu_hat), color = "coral1") +
  geom_smooth(data = estimate_mu, aes(x = mu, y = avg_mu_hat), se = FALSE, color = "deepskyblue") +
  geom_smooth(data = rejected_mu, aes(x = mu, y = avg_mu_hat), se = FALSE, color = "coral1") +
  labs(title = "True Mean (µ) vs Average Estimate Mean (µ^)", x = "True Mean (µ)", y = "Average Estimate Mean (µ^)")

mu_results_plot
```

When the true mean exceeds 4, the sample average of µ^ across tests that reject the null hypothesis closely approximates the actual true value of µ. In contrast, for true means less than 4, this sample average of µ^ does not match the true value of µ. This observed pattern arises because, when the true mean is greater than 4, both the statistical power and effect size are substantially high, nearing 1.

## Problem 3
### Raw Dataset
```{r message = FALSE}
homicide_df = read_csv("./homicide-data.csv") |>
  janitor::clean_names()
```

The dataset consists of `r nrow(homicide_df)` rows and `r ncol(homicide_df)` providing comprehensive information on homicide incidents across 50 large U.S. cities. Each observation includes essential victim characteristics, such as race, age, and gender. Additionally, each entry contains critical details about the circumstances of the homicide, including the incident date, geographic location (city and state), latitude and longitude coordinates, and the outcome of each incident, indicating whether the case was closed with or without an arrest.

### Data Cleaning
```{r}
homicide_df = homicide_df |>
  mutate(city_state = str_c(city, state, sep = ", ")) |> 
  group_by(city_state) |>
  summarize(total_homicides = n(),
            unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
            .groups = 'drop') |>
  filter(city_state != "Tulsa, AL") 
  
homicide_df
```
The raw data was further processed to add a `city_state` variable (e.g. “Baltimore, MD”). The dataset was then aggregated by city to calculate the total number of homicides and the number of unsolved homicides (classified as either `Closed without arrest` or `Open/No arrest`). 

The observation for Tulsa, AL was excluded from the analysis, as Tulsa is actually located in Oklahoma (OK), not Alabama (AL). This appeared to be a data entry error, which could lead to discrepancies in the upcoming analysis.

### Baltimore, MD
```{r}
baltimore_unsolved = homicide_df |>
  filter(city_state == "Baltimore, MD")

unsolved_count = pull(baltimore_unsolved, unsolved_homicides)
total_count = pull(baltimore_unsolved, total_homicides)

prop_test = prop.test(unsolved_count, total_count)

tidy_results = broom::tidy(prop_test)

estimate_prop = tidy_results$estimate
lower_ci = tidy_results$conf.low
upper_ci = tidy_results$conf.high
```
For the city of Baltimore, MD, a proportion test was conducted to estimate the proportion of unsolved homicides, yielding the following results:

* Estimated proportion: `r round(estimate_prop, 4)`
* Confidence intervals: [`r round(lower_ci, 4)`, `r round(upper_ci, 4)`]

### Proportion Test for All Cities
The following dataset and plot present the estimated proportions of unsolved homicides and their corresponding confidence intervals for 50 U.S. cities:
```{r warning = FALSE}
all_prop_test = homicide_df |>
  mutate(prop_test = purrr::map2(unsolved_homicides, total_homicides, ~prop.test(x = .x, n = .y)),
         tidy_result = purrr::map(prop_test, broom::tidy)) |>
  unnest(tidy_result) |>
  select(city_state, estimate, conf.low, conf.high)
  
all_prop_test
```

```{r}
estimate_plot = all_prop_test |>
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(x = "City, State", 
       y = "Estimated Proportion of Unsolved Homicides",
       title = "Estimated Proportion of Unsolved Homicides with Confidence Intervals") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

estimate_plot
```

This plot displays the estimated proportion of unsolved homicides in each city, with the length of the error bars representing the confidence intervals. 